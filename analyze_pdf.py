#!/usr/bin/env python3
"""
åˆ†æPDFæ–‡æ¡£ç»“æ„ï¼Œä¸ºåˆ†å—é…ç½®æä¾›ä¾æ®
"""

import PyPDF2
import re
from collections import Counter

def analyze_pdf_structure():
    """åˆ†æPDFæ–‡æ¡£ç»“æ„"""
    pdf_path = "çº¿ä¸‹åº—æ–‡æ¡£.pdf"
    
    print("ğŸ“Š PDFæ–‡æ¡£ç»“æ„åˆ†æ")
    print("=" * 50)
    
    try:
        with open(pdf_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            
            print(f"ğŸ“„ æ–‡æ¡£åŸºæœ¬ä¿¡æ¯:")
            print(f"  æ€»é¡µæ•°: {len(pdf_reader.pages)} é¡µ")
            
            # åˆ†ææ¯é¡µå†…å®¹
            page_stats = []
            all_text = ""
            
            for page_num, page in enumerate(pdf_reader.pages, 1):
                text = page.extract_text()
                if text.strip():
                    page_length = len(text)
                    page_stats.append({
                        'page': page_num,
                        'length': page_length,
                        'text': text
                    })
                    all_text += text + "\n"
                    
                    print(f"  ç¬¬{page_num}é¡µ: {page_length} å­—ç¬¦")
            
            print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"  æ€»å­—ç¬¦æ•°: {len(all_text)}")
            print(f"  å¹³å‡é¡µé•¿åº¦: {len(all_text) // len(page_stats)} å­—ç¬¦")
            print(f"  æœ€é•¿é¡µ: {max(p['length'] for p in page_stats)} å­—ç¬¦")
            print(f"  æœ€çŸ­é¡µ: {min(p['length'] for p in page_stats)} å­—ç¬¦")
            
            # åˆ†æå†…å®¹ç»“æ„
            analyze_content_structure(all_text)
            
            return page_stats, all_text
            
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        return None, None

def analyze_content_structure(text):
    """åˆ†æå†…å®¹ç»“æ„"""
    print(f"\nğŸ” å†…å®¹ç»“æ„åˆ†æ:")
    
    # æŸ¥æ‰¾ç« èŠ‚æ ‡é¢˜
    chapter_patterns = [
        r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« èŠ‚]',
        r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ã€.][^ã€‚\n]+',
        r'[A-Z][A-Z\s]+',
        r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[.ã€]\s*[^ã€‚\n]+'
    ]
    
    chapters = []
    for pattern in chapter_patterns:
        matches = re.findall(pattern, text)
        chapters.extend(matches)
    
    print(f"  å‘ç°ç« èŠ‚æ ‡è®°: {len(chapters)} ä¸ª")
    if chapters:
        print("  ä¸»è¦ç« èŠ‚:")
        for i, chapter in enumerate(chapters[:10], 1):
            print(f"    {i}. {chapter}")
    
    # åˆ†æå…³é”®è¯
    keywords = [
        'é€‰å€', 'æˆæœ¬', 'ç°é‡‘æµ', 'é£é™©', 'è¿è¥', 'ç®¡ç†', 'è´¢åŠ¡', 'ç›ˆåˆ©',
        'è£…ä¿®', 'äººå‘˜', 'åŸ¹è®­', 'è¥é”€', 'å®¢æˆ·', 'æœåŠ¡', 'è´¨é‡', 'æ ‡å‡†'
    ]
    
    print(f"\nğŸ“ˆ å…³é”®è¯é¢‘ç‡:")
    for keyword in keywords:
        count = text.count(keyword)
        if count > 0:
            print(f"  {keyword}: {count} æ¬¡")
    
    # åˆ†æå¥å­é•¿åº¦
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
    sentence_lengths = [len(s.strip()) for s in sentences if s.strip()]
    
    if sentence_lengths:
        avg_sentence_length = sum(sentence_lengths) / len(sentence_lengths)
        print(f"\nğŸ“ å¥å­é•¿åº¦åˆ†æ:")
        print(f"  å¹³å‡å¥å­é•¿åº¦: {avg_sentence_length:.1f} å­—ç¬¦")
        print(f"  æœ€é•¿å¥å­: {max(sentence_lengths)} å­—ç¬¦")
        print(f"  æœ€çŸ­å¥å­: {min(sentence_lengths)} å­—ç¬¦")

def suggest_chunk_config():
    """å»ºè®®åˆ†å—é…ç½®"""
    print(f"\nğŸ¯ åˆ†å—é…ç½®å»ºè®®:")
    print("=" * 50)
    
    # åŸºäºåˆ†æç»“æœå»ºè®®é…ç½®
    print("åŸºäºæ–‡æ¡£ç‰¹ç‚¹ï¼Œå»ºè®®ä»¥ä¸‹é…ç½®:")
    print()
    print("1. å°åˆ†å—é…ç½® (é€‚åˆç²¾ç¡®æ£€ç´¢):")
    print("   CHUNK_SIZE = 400-600")
    print("   CHUNK_OVERLAP = 50-100")
    print("   ä¼˜ç‚¹: æ£€ç´¢ç²¾åº¦é«˜ï¼Œé€‚åˆå…·ä½“é—®é¢˜")
    print("   ç¼ºç‚¹: ä¸Šä¸‹æ–‡å¯èƒ½ä¸å®Œæ•´")
    print()
    print("2. ä¸­ç­‰åˆ†å—é…ç½® (å¹³è¡¡é…ç½®):")
    print("   CHUNK_SIZE = 600-800")
    print("   CHUNK_OVERLAP = 100-150")
    print("   ä¼˜ç‚¹: å¹³è¡¡ç²¾åº¦å’Œä¸Šä¸‹æ–‡")
    print("   ç¼ºç‚¹: éœ€è¦æ›´å¤šå­˜å‚¨ç©ºé—´")
    print()
    print("3. å¤§åˆ†å—é…ç½® (é€‚åˆå¤æ‚é—®é¢˜):")
    print("   CHUNK_SIZE = 800-1200")
    print("   CHUNK_OVERLAP = 150-200")
    print("   ä¼˜ç‚¹: ä¸Šä¸‹æ–‡å®Œæ•´ï¼Œé€‚åˆç»¼åˆåˆ†æ")
    print("   ç¼ºç‚¹: æ£€ç´¢ç²¾åº¦å¯èƒ½é™ä½")
    print()
    print("ğŸ’¡ æ¨èé…ç½®:")
    print("   è€ƒè™‘åˆ°æ•™åŸ¹æ–‡æ¡£çš„ä¸“ä¸šæ€§å’Œå®Œæ•´æ€§ï¼Œå»ºè®®ä½¿ç”¨ä¸­ç­‰åˆ†å—é…ç½®")
    print("   CHUNK_SIZE = 700")
    print("   CHUNK_OVERLAP = 120")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” PDFæ–‡æ¡£åˆ†æå·¥å…·")
    print("=" * 50)
    
    # åˆ†æPDFç»“æ„
    page_stats, all_text = analyze_pdf_structure()
    
    if page_stats and all_text:
        # å»ºè®®é…ç½®
        suggest_chunk_config()
        
        print(f"\nğŸ“‹ å½“å‰é…ç½®:")
        print(f"  CHUNK_SIZE = 800")
        print(f"  CHUNK_OVERLAP = 100")
        print()
        print("ğŸ”„ æ˜¯å¦è¦æ›´æ–°é…ç½®ï¼Ÿ")

if __name__ == "__main__":
    main()
