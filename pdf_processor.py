import PyPDF2
import re
from typing import List, Dict
from config import Config

class PDFProcessor:
    def __init__(self, pdf_path: str):
        self.pdf_path = pdf_path
        self.chunk_size = Config.CHUNK_SIZE
        self.chunk_overlap = Config.CHUNK_OVERLAP
    
    def extract_text(self) -> str:
        """æå–PDFæ–‡æ¡£çš„æ–‡æœ¬å†…å®¹"""
        text = ""
        try:
            with open(self.pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                print(f"ğŸ“„ PDFæ–‡æ¡£ä¿¡æ¯: {len(pdf_reader.pages)} é¡µ")
                
                for page_num, page in enumerate(pdf_reader.pages, 1):
                    page_text = page.extract_text()
                    if page_text.strip():
                        text += f"\n=== ç¬¬{page_num}é¡µ ===\n{page_text}\n"
                        print(f"âœ… ç¬¬{page_num}é¡µæå–æˆåŠŸï¼Œé•¿åº¦: {len(page_text)} å­—ç¬¦")
                    else:
                        print(f"âš ï¸ ç¬¬{page_num}é¡µå†…å®¹ä¸ºç©º")
                        
        except Exception as e:
            print(f"âŒ PDFå¤„ç†é”™è¯¯: {e}")
            return ""
        return text
    
    def clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬å†…å®¹ï¼ˆä¿ç•™æ¢è¡Œï¼Œä¾¿äºæŒ‰å°èŠ‚/æ ‡é¢˜åˆ‡åˆ†ï¼‰"""
        print("ğŸ§¹ å¼€å§‹æ¸…ç†æ–‡æœ¬...")

        # ç»Ÿä¸€æ¢è¡Œç¬¦
        text = text.replace('\r\n', '\n').replace('\r', '\n')

        # ç§»é™¤é¡µé¢æ ‡è®°
        text = re.sub(r'\n?=== ç¬¬\d+é¡µ ===\n?', '\n', text)

        # è¡Œå†…å¤šä½™ç©ºç™½æ”¶ç¼©ï¼Œä½†ä¿ç•™æ¢è¡Œ
        text = re.sub(r'[ \t]+', ' ', text)

        # ç§»é™¤ä¸¤ç«¯ç©ºç™½çš„ç©ºè¡Œï¼Œå¹¶æ”¶æ•›è¿ç»­ç©ºè¡Œä¸ºå•ä¸ªç©ºè¡Œ
        # å…ˆé€è¡Œstripï¼Œå†é‡æ–°æ‹¼æ¥
        lines = [line.strip() for line in text.split('\n')]
        text = '\n'.join(lines)
        text = re.sub(r'\n{3,}', '\n\n', text)

        cleaned_text = text.strip('\n ')
        print(f"âœ… æ–‡æœ¬æ¸…ç†å®Œæˆï¼Œæ€»é•¿åº¦: {len(cleaned_text)} å­—ç¬¦")
        return cleaned_text

    def _is_heading(self, line: str) -> bool:
        """åŸºäºå¯å‘å¼è§„åˆ™åˆ¤æ–­ä¸€è¡Œæ˜¯å¦ä¸ºæ ‡é¢˜/å°èŠ‚ã€‚
        é€‚é…å¸¸è§ä¸­æ–‡ä¸æ•°å­—ç¼–å·æ ‡é¢˜ï¼šå¦‚â€œ1 æ¦‚è¿°â€ã€â€œ1.2 ç›®æ ‡â€ã€â€œä¸€ã€èƒŒæ™¯â€ã€â€œï¼ˆä¸€ï¼‰ç°çŠ¶â€ã€â€œç¬¬3ç«  æ–¹æ³•â€ç­‰ã€‚
        """
        if not line:
            return False

        candidate = line.strip()
        if len(candidate) == 0:
            return False

        # çŸ­è¡Œæ›´å¯èƒ½æ˜¯æ ‡é¢˜
        is_relatively_short = len(candidate) <= 30

        patterns = [
            r'^\d{1,3}(?:[\.|ï¼]\d{1,3}){0,3}[\sã€).)]+',            # 1 / 1.1 / 1.1.1
            r'^[ï¼ˆ\(]?\d{1,3}[ï¼‰\)]',                                 # ï¼ˆ1ï¼‰
            r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡]+ã€',                             # ä¸€ã€ äºŒã€ â€¦
            r'^[ï¼ˆ\(][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡]+[ï¼‰\)]',                 # ï¼ˆä¸€ï¼‰
            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡0-9]+[ç« èŠ‚éƒ¨åˆ†å·]',               # ç¬¬ä¸‰ç«  / ç¬¬1èŠ‚
        ]

        # ä»¥å†’å·/ï¼šç»“å°¾çš„çŸ­è¡Œä¹Ÿå¤šä¸ºæ ‡é¢˜
        looks_like_title_suffix = is_relatively_short and candidate.endswith((':', 'ï¼š'))

        for p in patterns:
            if re.match(p, candidate):
                return True

        # æ— å¥æœ«ç»ˆæ­¢ç¬¦å·ä¸”è¾ƒçŸ­ï¼Œä¸”ä¸ä»¥å¥å·ç»“æŸï¼Œå¯èƒ½ä¸ºæ ‡é¢˜
        no_sentence_ending = not re.search(r'[ã€‚ï¼ï¼Ÿ.!?ï¼›;]$', candidate)
        return is_relatively_short and (looks_like_title_suffix or no_sentence_ending)

    def _split_into_sections(self, text: str) -> List[Dict]:
        """æŒ‰æ ‡é¢˜/å°èŠ‚æ‹†åˆ†æ–‡æœ¬ï¼Œè¿”å› [{title, text}]ã€‚è‹¥æœªæ£€æµ‹åˆ°æ ‡é¢˜ï¼Œåˆ™è¿”å›å•èŠ‚ã€‚"""
        lines = [line for line in text.split('\n') if line is not None]
        sections: List[Dict] = []

        current_title = 'æ­£æ–‡'
        current_lines: List[str] = []
        detected_any_heading = False

        for line in lines:
            if self._is_heading(line):
                # åˆ·æ–°ä¸Šä¸€èŠ‚
                if current_lines:
                    sections.append({'title': current_title, 'text': '\n'.join(current_lines).strip()})
                current_title = line.strip()
                current_lines = []
                detected_any_heading = True
            else:
                current_lines.append(line)

        # æ”¶å°¾
        if current_lines:
            sections.append({'title': current_title, 'text': '\n'.join(current_lines).strip()})

        if not detected_any_heading:
            return [{'title': 'å…¨æ–‡', 'text': text.strip()}]

        return sections

    def _chunk_section(self, section_text: str, section_title: str, section_index: int) -> List[Dict]:
        """åœ¨å•ä¸ªå°èŠ‚å†…æŒ‰å¥å­+é•¿åº¦è¿›è¡Œåˆ†å—ï¼Œå¹¶ä¿ç•™é‡å ã€‚"""
        # å¥å­åˆ‡åˆ†ï¼Œä¿ç•™ç»ˆæ­¢ç¬¦
        parts = re.split(r'([ã€‚ï¼ï¼Ÿ!?ï¼›;])', section_text)
        sentences: List[str] = []
        for i in range(0, len(parts), 2):
            if i < len(parts):
                sentence = parts[i].strip()
                if i + 1 < len(parts):
                    sentence += parts[i + 1]
                if sentence:
                    sentences.append(sentence)

        # å¦‚æœæœªèƒ½æŒ‰å¥å­åˆ‡å‡ºï¼Œåˆ™å›é€€ä¸ºæŒ‰è¡Œ
        if not sentences:
            sentences = [s for s in section_text.split('\n') if s.strip()]

        chunks: List[Dict] = []
        current_chunk = ''
        chunk_id = 0

        for sentence in sentences:
            if len(current_chunk) + len(sentence) > self.chunk_size and current_chunk:
                chunks.append({
                    'text': current_chunk.strip(),
                    'chunk_id': chunk_id,
                    'length': len(current_chunk),
                    'section_title': section_title,
                    'section_index': section_index,
                })
                chunk_id += 1

                # é‡å 
                overlap_start = max(0, len(current_chunk) - self.chunk_overlap)
                current_chunk = current_chunk[overlap_start:] + sentence
            else:
                current_chunk += sentence

        if current_chunk.strip():
            chunks.append({
                'text': current_chunk.strip(),
                'chunk_id': chunk_id,
                'length': len(current_chunk),
                'section_title': section_title,
                'section_index': section_index,
            })

        return chunks
    
    def split_into_chunks(self, text: str) -> List[Dict]:
        """ä¼˜å…ˆæŒ‰æ ‡é¢˜/å°èŠ‚åˆ‡åˆ†ï¼Œå†åœ¨å°èŠ‚å†…æŒ‰é•¿åº¦åˆå¹¶ï¼Œä¿ç•™é‡å ã€‚"""
        print("âœ‚ï¸ å¼€å§‹åˆ†å‰²æ–‡æœ¬å—ï¼ˆç»“æ„åŒ–ä¼˜å…ˆï¼‰...")

        sections = self._split_into_sections(text)
        all_chunks: List[Dict] = []

        for idx, sec in enumerate(sections):
            section_chunks = self._chunk_section(sec['text'], sec['title'], idx)
            all_chunks.extend(section_chunks)

        print(f"âœ… æ–‡æœ¬åˆ†å‰²å®Œæˆï¼Œå…±ç”Ÿæˆ {len(all_chunks)} ä¸ªæ–‡æœ¬å—ï¼ˆ{len(sections)} ä¸ªå°èŠ‚ï¼‰")
        return all_chunks
    
    def process_document(self) -> List[Dict]:
        """å¤„ç†æ•´ä¸ªæ–‡æ¡£"""
        print("ğŸ”„ å¼€å§‹å¤„ç†PDFæ–‡æ¡£...")
        print(f"ğŸ“ æ–‡æ¡£è·¯å¾„: {self.pdf_path}")
        
        raw_text = self.extract_text()
        print(raw_text)
        
        if not raw_text:
            print("âŒ æ— æ³•æå–PDFå†…å®¹")
            return []
        
        print(f"ğŸ“Š åŸå§‹æ–‡æœ¬é•¿åº¦: {len(raw_text)} å­—ç¬¦")
        
        cleaned_text = self.clean_text(raw_text)
        
        if not cleaned_text:
            print("âŒ æ–‡æœ¬æ¸…ç†åä¸ºç©º")
            return []
        
        chunks = self.split_into_chunks(cleaned_text)
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªå—çš„é¢„è§ˆ
        print("\nğŸ“‹ æ–‡æœ¬å—é¢„è§ˆ:")
        for i, chunk in enumerate(chunks[:3]):
            preview = chunk['text'][:100] + "..." if len(chunk['text']) > 100 else chunk['text']
            sec = f" | å°èŠ‚: {chunk.get('section_title', 'N/A')}"
            print(f"  å—{i+1}{sec}: {preview}")
        
        if len(chunks) > 3:
            print(f"  ... è¿˜æœ‰ {len(chunks) - 3} ä¸ªæ–‡æœ¬å—")
        
        print(f"\nâœ… æ–‡æ¡£å¤„ç†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å—")
        return chunks

if __name__ == "__main__":
    processor = PDFProcessor(Config.PDF_PATH)
    chunks = processor.process_document()
    print(f"\nğŸ¯ æœ€ç»ˆç»“æœ: å…± {len(chunks)} ä¸ªæ–‡æœ¬å—")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    if chunks:
        total_length = sum(chunk['length'] for chunk in chunks)
        avg_length = total_length / len(chunks)
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  æ€»å­—ç¬¦æ•°: {total_length}")
        print(f"  å¹³å‡å—é•¿åº¦: {avg_length:.1f} å­—ç¬¦")
        print(f"  æœ€é•¿å—: {max(chunk['length'] for chunk in chunks)} å­—ç¬¦")
        print(f"  æœ€çŸ­å—: {min(chunk['length'] for chunk in chunks)} å­—ç¬¦")
